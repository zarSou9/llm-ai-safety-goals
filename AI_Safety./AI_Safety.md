### Description

Mitigate the risk that people build an agentic AI system which results in the loss of human control, extinction or some other existential catastrophe.

### Order

1. Paper: "The alignment problem from a deep learning perspective"
2. Paper: "Provably safe systems: the only path to controllable AGI"
