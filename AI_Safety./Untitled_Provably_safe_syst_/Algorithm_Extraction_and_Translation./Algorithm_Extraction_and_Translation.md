### Description

Establish reliable methods to convert black-box AI systems into transparent, verifiable code that preserves their capabilities. This includes both extracting learned algorithms from neural networks and synthesizing equivalent provably safe implementations.

### Questions

- How can we develop automated methods to identify and extract the minimal set of neurons/weights that are truly necessary for implementing a specific capability in a neural network, while proving that the removed components are genuinely redundant?

- What mathematical frameworks can we develop to formally verify that a synthesized symbolic program maintains semantic equivalence with the original neural network across the full input domain, not just observed examples?

- How can we automatically discover and extract reusable 'computational primitives' from neural networks that serve as building blocks across multiple tasks, similar to how human programmers use standard library functions?

- What techniques can we develop to automatically identify and characterize the failure modes of extracted symbolic programs compared to their neural network counterparts, especially for edge cases and out-of-distribution inputs?

- How can we develop methods to extract not just the computational logic but also the learned priors and inductive biases from neural networks into explicit, verifiable form?

- What approaches can we develop to automatically identify and extract the causal mechanisms by which a neural network implements abstract reasoning capabilities, rather than just pattern matching?

- How can we create techniques to automatically discover and extract the hierarchical decomposition of complex tasks that a neural network has learned, revealing how it breaks down problems into simpler subproblems?

### Order

1. Paper: "Opening the AI black box: program synthesis via mechanistic interpretability"
2. Paper: "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?"
3. Paper: "Abstraction and Symbolic Execution of Deep Neural Networks with Bayesian Approximation of Hidden Features"
4. Paper: "Verified Lifting of Deep learning Operators"
5. Paper: "Efficient compilation of expressive problem space specifications to neural network solvers"
6. Paper: "Deep Distilling: automated code generation using explainable deep learning"
7. Paper: "NEUROSPF: A Tool for the Symbolic Analysis of Neural Networks"
8. Paper: "Learning Transformer Programs"
9. Paper: "AnyCBMs: How to Turn Any Black Box into a Concept Bottleneck Model"
10. Paper: "Decompiling x86 Deep Neural Network Executables"
11. Paper: "Neuro-Symbolic Execution of Generic Source Code"
12. Paper: "Learning to Reverse DNNs from AI Programs Automatically"
13. Paper: "Automatic Discovery of Visual Circuits"
14. Paper: "Closed-form interpretation of neural network classifiers with symbolic gradients"
15. Paper: "Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition"
16. Paper: "Using Degeneracy in the Loss Landscape for Mechanistic Interpretability"
17. Paper: "Hide-and-Seek: A Template for Explainable AI"
18. Paper: "Inducing Causal Structure for Interpretable Neural Networks"
19. Paper: "Compact Proofs of Model Performance via Mechanistic Interpretability"
20. Paper: "Mechanistic Interpretability for AI Safety - A Review"
21. Paper: "InversionView: A General-Purpose Method for Reading Information from Neural Activations"
22. Paper: "How to Explain Neural Networks: an Approximation Perspective"
23. Paper: "Distilling Interpretable Models into Human-Readable Code"
24. Paper: "Learning outside the Black-Box: The pursuit of interpretable models"
25. Paper: "Extracting explanations, justification, and uncertainty from black-box deep neural networks"
26. Paper: "Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach"
27. Paper: "Unifying and Verifying Mechanistic Interpretations: A Case Study with Group Operations"
28. Paper: "Towards Automated Circuit Discovery for Mechanistic Interpretability"
