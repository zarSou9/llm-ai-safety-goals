### Paper

```json
{
	"id": "https://arxiv.org/abs/2405.16508",
	"arxiv_id": "2405.16508",
	"url": "https://arxiv.org/abs/2405.16508",
	"title": "AnyCBMs: How to Turn Any Black Box into a Concept Bottleneck Model",
	"published_date": "2024-05-26T00:00:00.000Z",
	"abstract": "Interpretable deep learning aims at developing neural architectures whose decision-making processes could be understood by their users. Among these techniqes, Concept Bottleneck Models enhance the interpretability of neural networks by integrating a layer of human-understandable concepts. These models, however, necessitate training a new model from the beginning, consuming significant resources and failing to utilize already trained large models. To address this issue, we introduce\"AnyCBM\", a method that transforms any existing trained model into a Concept Bottleneck Model with minimal impact on computational resources. We provide both theoretical and experimental insights showing the effectiveness of AnyCBMs in terms of classification performances and effectivenss of concept-based interventions on downstream tasks.",
	"citation_count": 0,
	"influential_citation_count": 0,
	"ref": "68560"
}
```

### Explanation

This paper presents a method to convert existing trained neural networks into more interpretable models by adding a layer of human-understandable concepts, which is relevant to algorithm extraction as it provides a way to make black-box AI systems more transparent and analyzable without requiring retraining from scratch. The approach could be a stepping stone toward extracting and understanding the learned algorithms within neural networks, though it falls short of full algorithm extraction or translation to verifiable code.
