### Paper

```json
{
	"id": "https://arxiv.org/abs/2403.08652",
	"arxiv_id": "2403.08652",
	"url": "https://arxiv.org/abs/2403.08652",
	"title": "Extracting explanations, justification, and uncertainty from black-box deep neural networks",
	"published_date": "2024-03-13T00:00:00.000Z",
	"abstract": "Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs.",
	"citation_count": 1,
	"influential_citation_count": 0,
	"ref": "85348"
}
```

### Explanation

This paper proposes a Bayesian method to extract explanations and uncertainty estimates from existing deep neural networks without retraining them, which partially addresses the sub-goal by making black-box AI systems more transparent and interpretable, though it falls short of fully converting them into verifiable code that preserves their capabilities.
