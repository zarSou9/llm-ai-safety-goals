### Paper

```json
{
	"id": "https://arxiv.org/abs/2105.07831",
	"arxiv_id": "2105.07831",
	"url": "https://arxiv.org/abs/2105.07831",
	"title": "How to Explain Neural Networks: an Approximation Perspective",
	"published_date": "2021-05-17T00:00:00.000Z",
	"abstract": "The lack of interpretability has hindered the large-scale adoption of AI technologies. However, the fundamental idea of interpretability, as well as how to put it into practice, remains unclear. We provide notions of interpretability based on approximation theory in this study. We first implement this approximation interpretation on a specific model (fully connected neural network) and then propose to use MLP as a universal interpreter to explain arbitrary black-box models. Extensive experiments demonstrate the effectiveness of our approach.",
	"citation_count": 1,
	"influential_citation_count": 0,
	"ref": "70892"
}
```

### Explanation

This paper proposes using approximation theory and multilayer perceptrons (MLPs) as a universal approach to interpret and explain the behavior of black-box neural networks, which relates to the sub-goal by offering a potential method for making AI systems more transparent, though it doesn't fully address the challenge of extracting or translating the underlying algorithms into verifiable code.
