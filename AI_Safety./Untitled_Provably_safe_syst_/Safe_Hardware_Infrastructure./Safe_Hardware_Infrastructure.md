### Description

Create a global infrastructure of provably secure hardware that can enforce safety constraints on AI systems. This hardware must be tamper-proof against even superintelligent adversaries and maintain security when networked together.

### Questions

- How can we design and verify hardware-level mechanisms that can reliably detect and respond to attempts by an AI system to modify its own computational substrate, even if those modification attempts leverage yet-unknown physics or engineering principles?

- What are the fundamental theoretical limits on creating tamper-proof hardware given quantum mechanics and other physical laws, and how close can we get to these limits with practical engineering approaches?

- How can we develop formal verification methods that can prove properties about hardware security even when the verifier is less intelligent than the potential adversary trying to compromise the system?

- What novel architectures could enable secure hardware to maintain its safety properties even when composed into large distributed networks, without creating new attack surfaces at the network coordination layer?

- How can we design hardware-enforced 'cognitive firewalls' that reliably prevent an AI system from accessing or influencing certain protected computational processes, even if the AI system has a superior understanding of the hardware's underlying physics?

- What mechanisms could allow secure hardware to maintain its safety properties even during hardware updates and maintenance, while preventing these necessary modification capabilities from becoming attack vectors?

- How can we develop hardware-based commitment mechanisms that would allow an AI system to credibly bind itself to certain constraints in a way that even its future more capable versions cannot circumvent?

- What novel approaches could enable secure hardware to detect and respond to attempts at social engineering or other indirect manipulation of human operators, without requiring the hardware itself to have sophisticated models of human psychology?

### Order

1. Paper: "Minimizing Trust with Exclusively-Used Physically-Isolated Hardware"
2. Paper: "GuardNN: secure accelerator architecture for privacy-preserving deep learning"
