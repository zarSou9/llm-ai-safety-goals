### Description

Develop formal specifications that fully capture all requirements for preventing catastrophic outcomes from AGI systems. These specifications must be both mathematically precise and completely cover all potential failure modes while remaining tractable for verification.

### Questions

- How can we formally specify and verify safety properties that remain robust even when an AI system undergoes significant capability jumps or self-modification, while avoiding the pitfall of specifications that become invalid above certain capability thresholds?

- What mathematical frameworks could allow us to precisely specify and verify safety properties across different levels of abstraction - from low-level compute to high-level behaviors - while maintaining formal guarantees about their relationships and compositions?

- How can we develop specification approaches that are robust to 'unknown unknowns' - formally capturing safety requirements even for failure modes and capabilities that we cannot currently anticipate or understand?

- What formal methods could allow us to specify and verify safety properties that hold across different possible 'paths to AGI' (e.g. scaling current architectures vs novel paradigms), while remaining tractable enough for practical verification?

- How can we create specifications that formally capture and preserve human values and preferences while being robust to potential changes in those values over time and across different human populations?

- What mathematical approaches would allow us to formally specify safety requirements that remain meaningful and enforceable even as an AI system's world model and ontology diverge significantly from human concepts?

- How can we develop formal specifications that prevent deceptive or manipulative behaviors while remaining robust to increasingly sophisticated forms of deception that we may not be able to anticipate?
