### Paper

```json
{
	"id": "https://arxiv.org/abs/2406.16220",
	"arxiv_id": "2406.16220",
	"url": "https://arxiv.org/abs/2406.16220",
	"title": "Learning Run-time Safety Monitors for Machine Learning Components",
	"published_date": "2024-06-23T00:00:00.000Z",
	"abstract": "For machine learning components used as part of autonomous systems (AS) in carrying out critical tasks it is crucial that assurance of the models can be maintained in the face of post-deployment changes (such as changes in the operating environment of the system). A critical part of this is to be able to monitor when the performance of the model at runtime (as a result of changes) poses a safety risk to the system. This is a particularly difficult challenge when ground truth is unavailable at runtime. In this paper we introduce a process for creating safety monitors for ML components through the use of degraded datasets and machine learning. The safety monitor that is created is deployed to the AS in parallel to the ML component to provide a prediction of the safety risk associated with the model output. We demonstrate the viability of our approach through some initial experiments using publicly available speed sign datasets.",
	"citation_count": 0,
	"influential_citation_count": 0,
	"ref": "18795"
}
```

### Explanation

This paper proposes a method for creating runtime safety monitors that can detect when ML models are operating unsafely after deployment due to changes in their environment, which is relevant to ensuring AI systems remain safe and under human control by providing early warning of potentially dangerous behavioral changes or performance degradation.
