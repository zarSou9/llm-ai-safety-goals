### Paper

```json
{
	"id": "https://arxiv.org/abs/2302.01928",
	"arxiv_id": "2302.01928",
	"url": "https://arxiv.org/abs/2302.01928",
	"title": "Aligning Human and Robot Representations",
	"published_date": "2023-04-13T00:00:00.000Z",
	"abstract": "To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behaviour. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.CCS CONCEPTS\u2022 Computing methodologies \u2192 Learning latent representations; Inverse reinforcement learning; Learning from demonstrations.",
	"citation_count": 6,
	"influential_citation_count": 0,
	"ref": "60005"
}
```

### Explanation

This paper addresses the challenge of ensuring that robots learn internal representations that align with human values and priorities when performing tasks, rather than just optimizing for functional efficiency. This directly relates to the AI safety sub-goal by exploring how to build AI systems whose internal models and decision-making processes remain aligned with human values, helping prevent misaligned behavior that could lead to loss of human control.
