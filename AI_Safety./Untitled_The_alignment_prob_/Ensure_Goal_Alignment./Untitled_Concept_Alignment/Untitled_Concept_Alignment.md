### Paper

```json
{
	"id": "http://arxiv.org/abs/2401.08672",
	"arxiv_id": "2401.08672",
	"url": "http://arxiv.org/abs/2401.08672",
	"title": "Concept Alignment",
	"published_date": "2024-01-09T00:00:00.000Z",
	"abstract": "Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.",
	"citation_count": 7,
	"influential_citation_count": 0,
	"ref": "32816"
}
```

### Explanation

This paper argues that before we can achieve value alignment between humans and AI systems, we must first ensure they share similar conceptual frameworks for understanding the world, as misaligned concepts could lead to misaligned values and potentially catastrophic outcomes even if we attempt to align values directly. This is relevant to AI safety as conceptual misalignment could be a key failure mode that leads to loss of control or unintended consequences, even if we believe we have successfully aligned an AI system's values with human values.
