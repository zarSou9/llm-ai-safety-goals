### Paper

```json
{
	"id": "https://arxiv.org/pdf/2112.10190v1.pdf",
	"arxiv_id": "2112.10190",
	"url": "https://arxiv.org/pdf/2112.10190v1.pdf",
	"title": "Demanding and Designing Aligned Cognitive Architectures",
	"published_date": "2021-12-19T00:00:00.000Z",
	"abstract": "With AI systems becoming more powerful and pervasive, there is increasing debate about keeping their actions aligned with the broader goals and needs of humanity. This multi-disciplinary and multi-stakeholder debate must resolve many issues, here we examine three of them. The first issue is to clarify what demands stakeholders might usefully make on the designers of AI systems, useful because the technology exists to implement them. We make this technical topic more accessible by using the framing of cognitive architectures. The second issue is to move beyond an analytical framing that treats useful intelligence as being reward maximization only. To support this move, we define several AI cognitive architectures that combine reward maximization with other technical elements designed to improve alignment. The third issue is how stakeholders should calibrate their interactions with modern machine learning researchers. We consider how current fashions in machine learning create a narrative pull that participants in technical and policy discussions should be aware of, so that they can compensate for it. We identify several technically tractable but currently unfashionable options for improving AI alignment.",
	"citation_count": 1,
	"influential_citation_count": 0,
	"ref": "20250"
}
```

### Explanation

This paper examines how to design AI systems with cognitive architectures that go beyond simple reward maximization to ensure better alignment with human values and goals, while also analyzing how stakeholders can effectively engage with ML researchers to implement these designs. This directly addresses AI safety by exploring concrete architectural approaches to prevent misaligned AI systems that could lead to loss of human control.
