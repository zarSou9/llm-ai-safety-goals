### Paper

```json
{
	"id": "https://arxiv.org/abs/2406.11039",
	"arxiv_id": "2406.11039",
	"url": "https://arxiv.org/abs/2406.11039",
	"title": "Dynamic Normativity: Necessary and Sufficient Conditions for Value Alignment",
	"published_date": "2024-06-16T00:00:00.000Z",
	"abstract": "The critical inquiry pervading the realm of Philosophy, and perhaps extending its influence across all Humanities disciplines, revolves around the intricacies of morality and normativity. Surprisingly, in recent years, this thematic thread has woven its way into an unexpected domain, one not conventionally associated with pondering\"what ought to be\": the field of artificial intelligence (AI) research. Central to morality and AI, we find\"alignment\", a problem related to the challenges of expressing human goals and values in a manner that artificial systems can follow without leading to unwanted adversarial effects. More explicitly and with our current paradigm of AI development in mind, we can think of alignment as teaching human values to non-anthropomorphic entities trained through opaque, gradient-based learning techniques. This work addresses alignment as a technical-philosophical problem that requires solid philosophical foundations and practical implementations that bring normative theory to AI system development. To accomplish this, we propose two sets of necessary and sufficient conditions that, we argue, should be considered in any alignment process. While necessary conditions serve as metaphysical and metaethical roots that pertain to the permissibility of alignment, sufficient conditions establish a blueprint for aligning AI systems under a learning-based paradigm. After laying such foundations, we present implementations of this approach by using state-of-the-art techniques and methods for aligning general-purpose language systems. We call this framework Dynamic Normativity. Its central thesis is that any alignment process under a learning paradigm that cannot fulfill its necessary and sufficient conditions will fail in producing aligned systems.",
	"citation_count": 0,
	"influential_citation_count": 0,
	"ref": "46705"
}
```

### Explanation

This paper proposes a framework called "Dynamic Normativity" that establishes necessary and sufficient conditions for successfully aligning AI systems with human values, arguing that any alignment approach that fails to meet these conditions will not produce truly aligned systems. The paper is directly relevant to AI safety as it addresses the fundamental challenge of ensuring AI systems reliably learn and maintain human values, which is crucial for preventing misaligned behavior that could lead to loss of control or catastrophic outcomes.
