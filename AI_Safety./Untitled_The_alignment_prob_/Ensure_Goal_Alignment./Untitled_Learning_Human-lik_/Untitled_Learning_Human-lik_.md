### Paper

```json
{
	"id": "http://arxiv.org/abs/2312.14106",
	"arxiv_id": "2312.14106",
	"url": "http://arxiv.org/abs/2312.14106",
	"title": "Learning Human-like Representations to Enable Learning Human Values",
	"published_date": "2023-12-21T00:00:00.000Z",
	"abstract": "How can we build AI systems that can learn any set of individual human values both quickly and safely, avoiding causing harm or violating societal standards for acceptable behavior during the learning process? We explore the effects of representational alignment between humans and AI agents on learning human values. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization. We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values -- including ethics, honesty, and fairness -- training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action. Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.",
	"citation_count": 3,
	"influential_citation_count": 0,
	"ref": "98306"
}
```

### Explanation

This paper explores how giving AI systems human-like representations of the world can help them learn human values more safely and effectively, demonstrating that when AI systems understand concepts similarly to humans, they can better learn and generalize human values while avoiding harmful actions during the learning process. This directly addresses AI safety by investigating methods to ensure AI systems can safely learn and align with human values, reducing risks of misaligned behavior.
