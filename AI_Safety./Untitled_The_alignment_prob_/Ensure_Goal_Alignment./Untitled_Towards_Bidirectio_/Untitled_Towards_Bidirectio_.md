### Paper

```json
{
	"id": "https://arxiv.org/abs/2406.09264",
	"arxiv_id": "2406.09264",
	"url": "https://arxiv.org/abs/2406.09264",
	"title": "Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions",
	"published_date": "2024-06-13T00:00:00.000Z",
	"abstract": "Recent advancements in general-purpose AI have highlighted the importance of guiding AI systems towards the intended goals, ethical principles, and values of individuals and groups, a concept broadly recognized as alignment. However, the lack of clarified definitions and scopes of human-AI alignment poses a significant obstacle, hampering collaborative efforts across research domains to achieve this alignment. In particular, ML- and philosophy-oriented alignment research often views AI alignment as a static, unidirectional process (i.e., aiming to ensure that AI systems' objectives match humans) rather than an ongoing, mutual alignment problem. This perspective largely neglects the long-term interaction and dynamic changes of alignment. To understand these gaps, we introduce a systematic review of over 400 papers published between 2019 and January 2024, spanning multiple domains such as Human-Computer Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML). We characterize, define and scope human-AI alignment. From this, we present a conceptual framework of\"Bidirectional Human-AI Alignment\"to organize the literature from a human-centered perspective. This framework encompasses both 1) conventional studies of aligning AI to humans that ensures AI produces the intended outcomes determined by humans, and 2) a proposed concept of aligning humans to AI, which aims to help individuals and society adjust to AI advancements both cognitively and behaviorally. Additionally, we articulate the key findings derived from literature analysis, including literature gaps and trends, human values, and interaction techniques. To pave the way for future studies, we envision three key challenges and give recommendations for future research.",
	"citation_count": 2,
	"influential_citation_count": 0,
	"ref": "38386"
}
```

### Explanation

This paper reviews over 400 papers to propose a bidirectional framework for human-AI alignment that considers both aligning AI systems with human values and helping humans adapt to AI advancements, which is relevant to AI safety by addressing how to maintain meaningful human control and ensure AI systems remain aligned with human interests even as both humans and AI systems evolve through interaction.
