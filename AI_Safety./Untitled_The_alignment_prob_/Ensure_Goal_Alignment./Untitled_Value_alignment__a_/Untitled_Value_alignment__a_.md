### Paper

```json
{
	"id": "https://arxiv.org/abs/2110.09240",
	"arxiv_id": "2110.09240",
	"url": "https://arxiv.org/abs/2110.09240",
	"title": "Value alignment: a formal approach",
	"published_date": "2023-02-06T00:00:00.000Z",
	"abstract": "principles that should govern autonomous AI systems. It essentially states that a system's goals and behaviour should be aligned with human values. But how to ensure value alignment? In this paper we first provide a formal model to represent values through preferences and ways to compute value aggregations; i.e. preferences with respect to a group of agents and/or preferences with respect to sets of values. Value alignment is then defined, and computed, for a given norm with respect to a given value through the increase/decrease that it results in the preferences of future states of the world. We focus on norms as it is norms that govern behaviour, and as such, the alignment of a given system with a given value will be dictated by the norms the system follows.",
	"citation_count": 31,
	"influential_citation_count": 2,
	"ref": "46439"
}
```

### Explanation

This paper proposes a formal mathematical framework for representing human values as preferences and measuring how well an AI system's governing rules (norms) align with those values, which is directly relevant to ensuring AI systems behave in accordance with human values and preventing catastrophic misalignment. The approach focuses specifically on evaluating how a system's norms affect future states of the world relative to human preferences.
