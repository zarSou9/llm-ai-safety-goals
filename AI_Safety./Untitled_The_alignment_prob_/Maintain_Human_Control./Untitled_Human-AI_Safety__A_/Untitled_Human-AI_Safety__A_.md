### Paper

```json
{
	"id": "https://arxiv.org/abs/2405.09794",
	"arxiv_id": "2405.09794",
	"url": "https://arxiv.org/abs/2405.09794",
	"title": "Human-AI Safety: A Descendant of Generative AI and Control Systems Safety",
	"published_date": "2024-05-16T00:00:00.000Z",
	"abstract": "Artificial intelligence (AI) is interacting with people at an unprecedented scale, offering new avenues for immense positive impact, but also raising widespread concerns around the potential for individual and societal harm. Today, the predominant paradigm for human--AI safety focuses on fine-tuning the generative model's outputs to better agree with human-provided examples or feedback. In reality, however, the consequences of an AI model's outputs cannot be determined in isolation: they are tightly entangled with the responses and behavior of human users over time. In this paper, we distill key complementary lessons from AI safety and control systems safety, highlighting open challenges as well as key synergies between both fields. We then argue that meaningful safety assurances for advanced AI technologies require reasoning about how the feedback loop formed by AI outputs and human behavior may drive the interaction towards different outcomes. To this end, we introduce a unifying formalism to capture dynamic, safety-critical human--AI interactions and propose a concrete technical roadmap towards next-generation human-centered AI safety.",
	"citation_count": 2,
	"influential_citation_count": 0,
	"ref": "03176"
}
```

### Explanation

This paper argues that AI safety requires understanding and managing the dynamic feedback loops between AI systems and human behavior over time, rather than just focusing on fine-tuning AI outputs in isolation, and proposes a framework for analyzing these interactions to ensure sustained human control and safety. This is directly relevant to maintaining human control over AI systems as it addresses how human-AI interactions could lead to loss of control if not properly understood and managed.
