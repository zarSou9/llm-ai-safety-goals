### Paper

```json
{
	"id": "https://arxiv.org/abs/2203.04754",
	"arxiv_id": "2203.04754",
	"url": "https://arxiv.org/abs/2203.04754",
	"title": "System Cards for AI-Based Decision-Making for Public Policy",
	"published_date": "2023-02-06T00:00:00.000Z",
	"abstract": "Decisions impacting human lives are increasingly being made or assisted by automated decision-making algorithms. Many of these algorithms process personal data for predicting recidivism, credit risk analysis, identifying individuals using face recognition, and more. While potentially improving efficiency and effectiveness, such algorithms are not inherently free from bias, opaqueness, lack of explainability, maleficence, and the like. Given that the outcomes of these algorithms have a significant impact on individuals and society and are open to analysis and contestation after deployment, such issues must be accounted for before deployment. Formal audits are a way of ensuring algorithms meet the appropriate accountability standards. This work, based on an extensive analysis of the literature and an expert focus group study, proposes a unifying framework for a system accountability benchmark for formal audits of artificial intelligence-based decision-aiding systems. This work also proposes system cards to serve as scorecards presenting the outcomes of such audits. It consists of 56 criteria organized within a four-by-four matrix composed of rows focused on (i) data, (ii) model, (iii) code, (iv) system, and columns focused on (a) development, (b) assessment, (c) mitigation, and (d) assurance. The proposed system accountability benchmark reflects the state-of-the-art developments for accountable systems, serves as a checklist for algorithm audits, and paves the way for sequential work in future research.",
	"citation_count": 12,
	"influential_citation_count": 1,
	"ref": "96142"
}
```

### Explanation

This paper proposes a framework for auditing AI decision-making systems through "system cards" that evaluate 56 criteria across data, model, code and system dimensions to ensure accountability and detect potential issues before deployment. While focused on current AI systems rather than advanced agentic AI, the auditing framework could inform approaches for maintaining human oversight and control of AI systems as they become more capable.
