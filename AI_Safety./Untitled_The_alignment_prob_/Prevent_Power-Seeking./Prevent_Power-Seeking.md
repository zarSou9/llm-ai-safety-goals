### Description

Prevent AI systems from developing and executing strategies to gain control over key resources and decision-making capabilities at humanity's expense. This includes preventing both overt power-seeking behaviors and subtle forms that could emerge through instrumental convergence.

### Questions

- How can we develop formal metrics to quantify and compare the 'power-seeking potential' of different AI architectures and training approaches before deployment, similar to how we measure model performance?

- What are the fundamental mathematical relationships between an AI system's capability to generalize across domains and its propensity for power-seeking behavior? Can we identify architectural constraints that limit power-seeking while preserving beneficial generalization?

- How can we design AI systems that maintain a 'ceiling' on their maximum achievable influence over critical resources and decision-making processes, even as they undergo recursive self-improvement or architectural changes?

- What novel training techniques could create AI systems that actively prefer maintaining distributed power structures over centralized control, while still effectively pursuing their intended objectives?

- How can we develop reliable empirical tests to detect subtle forms of power-seeking behavior that might emerge during training, especially behaviors that could be precursors to more overt power-seeking?

- What are the theoretical bounds on our ability to create 'power-stable' AI systems - systems that maintain a constant level of influence over their environment regardless of increases in their capabilities?

- How can we design AI architectures that maintain their intended power limitations even when composed or combined with other AI systems, preventing the emergence of power-seeking behavior through system interaction?

### Order

1. Paper: "On Avoiding Power-Seeking by Artificial Intelligence"
2. Paper: "Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models"
3. Paper: "Intelligence and Unambitiousness Using Algorithmic Information Theory"
4. Paper: "Structured access: an emerging paradigm for safe AI deployment"
5. Paper: "The Benefits of Power Regularization in Cooperative Reinforcement Learning"
6. Paper: "Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective"
